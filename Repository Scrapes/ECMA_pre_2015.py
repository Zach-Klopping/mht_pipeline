#!/usr/bin/env python3
"""
ECONOMETRICA (ECMA) supplemental ZIP downloader â€” Dropbox integrated
Workflow:
- Reads an Excel file tracking articles with columns: ['title', 'url', 'coverDate', 'replication_package', 'supplementary_package'].
- Filters for rows where replication_package == 0 and supplementary_package == 0 and year < 2016.
- Visits each article landing page to locate supplemental ZIP files.
    - If a direct .zip link is found, downloads the file.
    - Attempts to match title with Excel file title.
- Uploads each downloaded ZIP to the appropriate Dropbox folder.
- Updates the Excel file to mark completed rows so future runs skip them.
Install:
    pip install pandas beautifulsoup4 regex rapidfuzz undetected-chromedriver selenium dropbox openpyxl

Note:
    Requires Chrome (matching CHROME_MAJOR), a valid Dropbox token,
    and an Excel file with article metadata.
"""

import time
import os
import shutil
import pandas as pd
import regex as re
import undetected_chromedriver as uc
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import dropbox
from dropbox.files import WriteMode
from rapidfuzz import process

# =========================
# CONFIG (edit these to run)
# =========================
EXCEL_PATH = '/Users/zachklopping/Desktop/John List/MHT/Downloaded Excels/Fully_Downloaded_Econometrica_2000-2025.xlsx'  # Excel file tracking downloads
CHROME_MAJOR      = 141                   
DROPBOX_TOKEN     = 'sl.u.AGBaVsyRlcarZeTQlSOJzsYXbqO9S14JzP8tYzRriBoOHalEwpE_lsi-HAn4x6i2CKrfq2C8BWK0ztRZockHfPwF4Q7Ssd3H8iuruvg2pUVjJnDk0HqUmNa_fwZhXsv0Hwcf1izp3u6Ec4Kl0eZ5miBe-lNGQ2AHUu1uBZFLRKDpw1jJOCCoOrfnj4u8FXaGcmiRslH-r--2NUlQ6dG8ZAzOH4Z__aQtRu2hdtSN87621Ib-2ZXJyyDCoS7ugf5RHYdC-drjLqhUeDS-TonRjfdqAfqr1yX2wqVHlL509DLfDYQcFGCQQBqFLJN7symgKVnfT8Z8Cup5eNDFyk2FtPmekP-hzPJJ6InNBnVweAh6yFDg7dsv2HQrE-NpRtjbJjy1BSAtQXTj8-IfOCj4cFkKk_pbzAvvcuKq7FGnhV_0hZsakqNmmI6oX41M3udUiPnJGfa3Tm88yQBlF4VoAFip4luA6ZBqS-ezzmXW_DL2qpy0I19tfL_mAfpSBohQk6VdvpxISS7SVmTFqNEU3G6mq8i5O2OtR9ngO2PfUU3BKYr2-E8QTE6S91px8M1iTf7ca7NNs7WJKEpFpz9Yw398guSC837ej6vvWl8Pe_wJDVA6JuVMmnGpF4SiQ3NvUBM3NHFpkwXmoJbRXa8K2G25IO7tAm9EFZmwIjCvellwhaGxDA2r64gWFycpjmGHJrszlHcbYRqui0gi8ULNgQsN8HKHBIopTczJJ93fKM-aC4DobaYwnwYtodUz-mE0Jfw3snZSEQEe7ozz69HhIj1wpl7oh_ZGfgckAgRiQwEA736XGoG5oAo4dvwkL5eexCuEnGp0Ebq44qsYcV1m5z5weeXJY7SqVDv4E0cItLfXXSQ39s6KNIKiB86JSxZ74NEdxr4GOsavR30l3_91P_7zw72i1qMcl0cvh6JJfT8GSdbMI83-pcApojutY3zHWrqfbfnxDUQwcb7xZcMg9Z3zB5NBYogVzb3XEAjCTp_SAwgcBad_PuaR7Gj3QiLzFwUUTYCoxf33tvCGVWRLF__3V2_WvV5O666btbm_oSHV0X5Jf2Qxc4mqodYLk7-3_IUr2smW7B-Lt9PY4BFPhhmAOgSEzh9jM-m7RpiRMAi3oevT9OuVvoxfXw8Ry7QiaAglstA2oVQMjRKQKmSOUHplzavP0CJ3ml-A2zGaiqhlJ3bKCsGyUg6AJYB6mdjS2HcNopVp2V_Nwsw71lTaZ5HEtrE6sIP0_J79J827lj9WiK0It_VARctAeEfKSrTCwou6oUtUBXozmE5dPMUGwBM77lHoRWCm9GX0e6jvrAroLxgUnV26mie9qknwWRa_Yevw_Na3PRMo17RxIDfmzM2BhYRlgWgyvzEzmTAEwfatYikYrkqRa8iNMKOx1ks6ynb8iUux6V15QOVt_sSR7Qmt8EXAd2Gv8Gcbb6xh53iQvA'  # Dropbox API token

DOWNLOAD_FOLDER = '/Users/zachklopping/Desktop/John List/MHT/Scrapes/ECMA Downloads'                         # Local staging folder for downloaded ZIPs
os.makedirs(DOWNLOAD_FOLDER, exist_ok=True)

# =========================
# Already Set Config (do NOT edit below)
# =========================
URL_COL           = "url"                  # Excel column with article landing page URL
TITLE_COL         = "title"                # Excel column with article title
FLAG_SUPP_COL     = "supplementary_package" # 0 = needs supplemental zip, 1 = done
START_YEAR        = 2010                  # first year to scan
END_YEAR          = 2015                  # last year to scan
DROPBOX_FOLDER    = "/MHT/ECMA Pre 2015 Supplementary Packages"       # Dropbox destination root (must start with "/")

if not DROPBOX_TOKEN:
    raise RuntimeError("Missing DROPBOX_TOKEN")

dbx = dropbox.Dropbox(DROPBOX_TOKEN)
CHUNK_SIZE = 8 * 1024 * 1024

def _ensure_dropbox_folder(folder_path: str):
    folder_path = (folder_path or "/").rstrip("/") or "/"
    if folder_path == "/": return
    try:
        dbx.files_create_folder_v2(folder_path)
    except dropbox.exceptions.ApiError:
        pass
_ensure_dropbox_folder(DROPBOX_FOLDER)

def _upload_file_to_dropbox(local_path: str, dropbox_folder: str, dest_name: str) -> str:
    dropbox_folder = dropbox_folder.rstrip("/")
    if not dropbox_folder.startswith("/"):
        dropbox_folder = "/" + dropbox_folder
    dropbox_path = f"{dropbox_folder}/{dest_name}"
    size = os.path.getsize(local_path)
    with open(local_path, "rb") as f:
        if size <= CHUNK_SIZE:
            dbx.files_upload(f.read(), dropbox_path, mode=WriteMode("add"), autorename=True, mute=True)
        else:
            start = dbx.files_upload_session_start(f.read(CHUNK_SIZE))
            cursor = dropbox.files.UploadSessionCursor(session_id=start.session_id, offset=f.tell())
            commit = dropbox.files.CommitInfo(path=dropbox_path, mode=WriteMode("add"), autorename=True, mute=True)
            while f.tell() < size:
                if (size - f.tell()) <= CHUNK_SIZE:
                    dbx.files_upload_session_finish(f.read(CHUNK_SIZE), cursor, commit)
                else:
                    dbx.files_upload_session_append_v2(f.read(CHUNK_SIZE), cursor)
                    cursor.offset = f.tell()
    return dropbox_path

# =========================
# Browser
# =========================
options = uc.ChromeOptions()
prefs = {"download.default_directory": DOWNLOAD_FOLDER}
options.add_experimental_option("prefs", prefs)
driver = uc.Chrome(options=options, version_main=141)

# =========================
# Data
# =========================
journal_data = pd.read_excel(EXCEL_PATH)
journal_data["coverDate"] = pd.to_datetime(journal_data["coverDate"], errors="coerce")
journal_data["year"] = journal_data["coverDate"].dt.year
if FLAG_SUPP_COL not in journal_data.columns:
    journal_data[FLAG_SUPP_COL] = 0

to_download = journal_data[
    journal_data[FLAG_SUPP_COL].fillna(0).astype(int) == 0
]

# =========================
# Helpers
# =========================
def clean_title_for_filename(t: str) -> str:
    return re.sub(r"[^A-Za-z0-9]+", "_", str(t)).strip("_") or "untitled"

def wait_for_file(download_dir: str, exts=(".zip",), timeout=600) -> str | None:
    start = time.time()
    seen = {os.path.join(download_dir, f) for f in os.listdir(download_dir)}
    while time.time() - start < timeout:
        if any(f.endswith(".crdownload") for f in os.listdir(download_dir)):
            time.sleep(1); continue
        new_files = []
        for f in os.listdir(download_dir):
            p = os.path.join(download_dir, f)
            if p in seen: continue
            if f.lower().endswith(exts):
                new_files.append(p)
        if new_files:
            newest = max(new_files, key=os.path.getctime)
            # check stability
            size1 = os.path.getsize(newest); time.sleep(3)
            size2 = os.path.getsize(newest)
            if size1 == size2:
                return newest
        time.sleep(2)
    return None

# =========================
# Main loop (year-month)
# =========================
for year in range(START_YEAR, END_YEAR + 1):
    for month in range(1, 13, 2):  # odd months
        url = f"https://www.econometricsociety.org/publications/econometrica/browse/supplemental-materials/issue-supplemental-materials/{year}/{month:02d}"
        print(f"\nProcessing {year}-{month:02d}: {url}")
        driver.get(url)
        try:
            WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, '//*[@id="cookie-notice"]/p[3]/a[2]'))
            ).click()
            print("âœ… Cookie banner accepted.")
            time.sleep(1)
        except (TimeoutException, NoSuchElementException):
            pass

        time.sleep(3)
        soup = BeautifulSoup(driver.page_source, "html.parser")

        zips = []
        for a in soup.find_all("a", href=True):
            if not a["href"].lower().endswith(".zip"):
                continue
            h3 = a.find_previous("h3")
            heading = h3.get_text(strip=True) if h3 else "untitled"
            zip_url = urljoin(driver.current_url, a["href"])
            zips.append({"heading": heading, "zip_url": zip_url})

        if not zips:
            print("âŒ No ZIP files found")
            continue

        for rec in zips:
            driver.get(rec["zip_url"])  # <---- you must go to the ZIP link first!
            start_url = driver.current_url
            file_path = wait_for_file(DOWNLOAD_FOLDER, exts=(".zip",), timeout=10)  # short early wait

            if not file_path:
                # still on the same page and no download started
                time.sleep(2)
                if driver.current_url == start_url:
                    print(f"âŒ No download triggered and still on {start_url} â€” skipping.")
                    driver.back()
                    time.sleep(2)
                    continue
                else:
                    # If URL changed maybe redirect to an error page â€” we can inspect or just skip
                    print(f"âŒ Redirected but no file for {rec['heading']} â€” skipping.")
                    driver.back()
                    time.sleep(2)
                    continue

            safe = clean_title_for_filename(rec["heading"])
            new_name = f"ECMA_{safe}.zip"
            target = os.path.join(DOWNLOAD_FOLDER, new_name)
            shutil.move(file_path, target)
            print(f"ðŸ“¥ Saved: {target}")

            def normalize(s: str) -> str:
                s = str(s).lower()
                s = re.sub(r'[_\-]+', ' ', s)       # underscores and hyphens to spaces
                s = re.sub(r'[^\w\s]', '', s)       # drop punctuation
                s = re.sub(r'\s+', ' ', s).strip()  # collapse spaces
                return s

            # --- fuzzy match ---
            parsed = normalize(rec["heading"])
            choices = [normalize(t) for t in journal_data["title"].astype(str)]
            best = process.extractOne(parsed, choices)

            # choose the Dropbox destination depending on match
            if best and best[1] >= 90:
                match_str, score, idx = best
                norm_titles = journal_data["title"].astype(str).apply(normalize)
                journal_data.loc[norm_titles == match_str, FLAG_SUPP_COL] = 1
                print(f"âœ… Marked supplementary for '{match_str}' (score {score})")
                dropbox_dest = DROPBOX_FOLDER
            else:
                print(f"âš ï¸ No good match for: {parsed}")
                # create /No Match subfolder inside your main ECMA folder
                dropbox_dest = f"{DROPBOX_FOLDER}/No Match"
                _ensure_dropbox_folder(dropbox_dest)

            # upload to the chosen Dropbox folder
            try:
                dbx_path = _upload_file_to_dropbox(target, dropbox_dest, new_name)
                print(f"âœ… Uploaded to Dropbox: {dbx_path}")
                os.remove(target)
            except Exception as e:
                print(f"Dropbox upload failed: {e}")

            # only save Excel if we matched
            if best and best[1] >= 90:
                journal_data.to_excel(EXCEL_PATH, index=False)
            time.sleep(2)

# =========================
# Cleanup
# =========================
driver.quit()
print("\nðŸŽ‰ All ECMA supplemental materials processed.")
