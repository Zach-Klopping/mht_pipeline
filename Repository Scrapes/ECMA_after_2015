#!/usr/bin/env python3
"""
ECONOMETRICA (ECMA) supplementary/replication ZIP downloader with Dropbox upload

Workflow:
- Reads an Excel file with columns: ['title', 'url', 'coverDate', 'replication_package', 'supplementary_package'].
- Filters for rows where replication_package == 0 and supplementary_package == 0 and year >= 2016.
- Visits each article landing page and tries to find a supplementary or replication link.
    - If link points directly to a ZIP: download and mark supplementary_package = 1.
    - If link points to Zenodo: follow to find direct ZIP and mark replication_package = 1.
- Uploads the downloaded ZIP file to Dropbox.
- Updates the Excel file so future runs skip completed rows.
Install:
    pip install pandas beautifulsoup4 regex undetected-chromedriver selenium dropbox openpyxl

Note:
    Requires Chrome (matching CHROME_MAJOR), a valid Dropbox token,
    and an Excel file with article metadata.
"""

import os
import time
import shutil
import pandas as pd
import regex as re
import undetected_chromedriver as uc
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import dropbox
from dropbox.files import WriteMode

# =========================
# USER CONFIG ‚Äî edit these
# =========================
EXCEL_PATH = "set here"  # Excel file tracking downloads
DOWNLOAD_FOLDER = "set here"                          # Local staging folder for downloaded ZIPs
os.makedirs(DOWNLOAD_FOLDER, exist_ok=True)

CHROME_MAJOR = 141
# =========================
# INTERNAL SETTINGS (no need to edit)
# =========================
DROPBOX_TOKEN = "your_dropbox_oauth_token"                           # Dropbox API token (keep secret)
DROPBOX_FOLDER_REPL = "/MHT/ECMA Replication Packages"
DROPBOX_FOLDER_SUPP = "/MHT/ECMA Supplementary Packages"

URL_COL           = "url"                   # Excel column with article landing page URL
TITLE_COL         = "title"                 # Excel column with article title
FLAG_REPL_COL     = "replication_package"   # 0 = needs dataset (Dataverse), 1 = done
FLAG_SUPP_COL     = "supplementary_package" # 0 = needs supplementary zip, 1 = done

YEAR_MIN = 2016   # only process rows with coverDate.year >= YEAR_MIN
CHUNK_SIZE = 8 * 1024 * 1024  # Dropbox upload chunk size (8 MB)

if not DROPBOX_TOKEN:
    raise RuntimeError("Missing DROPBOX_TOKEN for Dropbox API.")

dbx = dropbox.Dropbox(DROPBOX_TOKEN)

def _ensure_dropbox_folder(folder_path: str):
    """Create Dropbox folder if it doesn't exist (no error if it does)."""
    folder_path = (folder_path or "/").rstrip("/") or "/"
    if folder_path == "/":
        return
    try:
        dbx.files_create_folder_v2(folder_path)
    except dropbox.exceptions.ApiError:
        pass

def _upload_file_to_dropbox(local_path: str, dropbox_folder: str, dest_name: str) -> str:
    """Upload a file to Dropbox (chunked for large files). Returns the Dropbox path."""
    dropbox_folder = dropbox_folder.rstrip("/")
    if not dropbox_folder.startswith("/"):
        dropbox_folder = "/" + dropbox_folder
    dropbox_path = f"{dropbox_folder}/{dest_name}"
    size = os.path.getsize(local_path)

    with open(local_path, "rb") as f:
        if size <= CHUNK_SIZE:
            dbx.files_upload(f.read(), dropbox_path, mode=WriteMode("add"), autorename=True, mute=True)
        else:
            start = dbx.files_upload_session_start(f.read(CHUNK_SIZE))
            cursor = dropbox.files.UploadSessionCursor(session_id=start.session_id, offset=f.tell())
            commit = dropbox.files.CommitInfo(path=dropbox_path, mode=WriteMode("add"), autorename=True, mute=True)
            while f.tell() < size:
                if (size - f.tell()) <= CHUNK_SIZE:
                    dbx.files_upload_session_finish(f.read(CHUNK_SIZE), cursor, commit)
                else:
                    dbx.files_upload_session_append_v2(f.read(CHUNK_SIZE), cursor)
                    cursor.offset = f.tell()
    return dropbox_path

_ensure_dropbox_folder(DROPBOX_FOLDER_REPL)
_ensure_dropbox_folder(DROPBOX_FOLDER_SUPP)
# =========================
# Browser setup
# =========================
options = uc.ChromeOptions()
prefs = {
    "download.default_directory": DOWNLOAD_FOLDER,
    "plugins.always_open_pdf_externally": True,  # don‚Äôt preview PDFs, download instead
    "download.extensions_to_open": "applications/pdf",
}
options.add_experimental_option("prefs", prefs)
driver = uc.Chrome(options=options, version_main=CHROME_MAJOR)  # set to your Chrome major version

# =========================
# Load data & filter rows to process
# =========================
journal_data = pd.read_excel(EXCEL_PATH)

# Ensure required columns exist
if FLAG_REPL_COL not in journal_data.columns:
    journal_data[FLAG_REPL_COL] = 0
if FLAG_SUPP_COL not in journal_data.columns:
    journal_data[FLAG_SUPP_COL] = 0

journal_data["coverDate"] = pd.to_datetime(journal_data["coverDate"], errors="coerce")
journal_data["year"] = journal_data["coverDate"].dt.year

# Only rows needing download (both flags 0) and year >= YEAR_MIN
to_download = journal_data[
    (journal_data[FLAG_REPL_COL].fillna(0).astype(int) == 0)
    & (journal_data[FLAG_SUPP_COL].fillna(0).astype(int) == 0)
    & (journal_data["year"] >= YEAR_MIN)
]

# =========================
# Helper functions
# =========================
def wait_for_zip(download_dir: str, exts=(".pdf", ".zip"), timeout: int = 180) -> str | None:
    """Wait for a new PDF/ZIP file to appear and finish downloading in the folder."""
    start = time.time()
    seen = {os.path.join(download_dir, f) for f in os.listdir(download_dir)}
    while time.time() - start < timeout:
        if any(f.endswith(".crdownload") for f in os.listdir(download_dir)):
            time.sleep(1)
            continue
        files = []
        for f in os.listdir(download_dir):
            p = os.path.join(download_dir, f)
            if p in seen:
                continue
            if not f.lower().endswith(exts):
                continue
            if f.startswith("ECMA_"):  # skip already renamed files
                continue
            files.append(p)
        if files:
            return max(files, key=os.path.getctime)
        time.sleep(1)
    return None

def clean_title_for_filename(title: str) -> str:
    """Make a safe filename from an article title."""
    return re.sub(r"[^A-Za-z0-9]+", "_", str(title)).strip("_") or "untitled"

# =========================
# Main loop
# =========================
for orig_idx, row in to_download.iterrows():
    try:
        title = row.get("title", f"idx_{orig_idx}")
        url = row.get("url")

        if not isinstance(url, str) or not url.startswith("http"):
            print(f"[{orig_idx}] Skipping (bad/missing URL): {title}")
            continue

        print(f"\n[{orig_idx}] Processing: {title}")
        driver.get(url)

        # Try to accept cookie banner if present
        try:
            WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, '//*[@id="accept-button"]'))
            ).click()
            print("‚úÖ Cookies banner accepted.")
            time.sleep(1)
        except (TimeoutException, NoSuchElementException):
            print("No cookies banner.")

        # Pause if Cloudflare challenge appears
        try:
            WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.ID, "cf-challenge-running"))
            )
            print("‚ö†Ô∏è Cloudflare challenge detected. Complete in browser, then press ENTER.")
            input()
        except TimeoutException:
            pass

        time.sleep(3)

        # Find supplementary/replication links on page
        zip_url = None
        zip_elems = driver.find_elements(By.CSS_SELECTOR, "p.supp_link a.button")

        if zip_elems:
            for el in zip_elems:
                href = el.get_attribute("href")
                if not href:
                    continue
                href = urljoin(driver.current_url, href)

                if href.lower().endswith(".zip"):
                    zip_url = href
                    print("Found direct ZIP:", zip_url)
                    break

                if "zenodo" in href.lower():
                    zip_url = href
                    print("Found Zenodo link:", zip_url)
                    break
        else:
            print("No supplementary/replication link found.")

        if not zip_url:
            print(f"[{orig_idx}] ‚ùå No ZIP/Zenodo link found; skipping.")
            continue

        # If Zenodo: open and look for a direct ZIP button
        is_zenodo = "zenodo" in zip_url.lower()
        if is_zenodo:
            driver.get(zip_url)
            time.sleep(2)
            soup = BeautifulSoup(driver.page_source, "html.parser")
            real_link = soup.find("a", class_="ui compact mini button right floated archive-link")
            if not real_link:
                real_link = soup.find("a", href=re.compile(r"\.zip$", re.I))

            if real_link:
                zip_url = real_link["href"]
                if not zip_url.startswith("http"):
                    zip_url = urljoin(driver.current_url, zip_url)
                print("Zenodo direct ZIP URL:", zip_url)
            else:
                print("‚ö†Ô∏è Could not find ZIP download link on Zenodo page; manual check needed.")
                continue

        # Download file
        driver.get(zip_url)

        # Wait for download to complete
        downloaded_path = wait_for_zip(DOWNLOAD_FOLDER)
        if not downloaded_path or not os.path.exists(downloaded_path):
            print(f"[{orig_idx}] ‚ùå Download failed or timed out.")
            continue

        # Rename to safe filename
        safe_title = clean_title_for_filename(title)
        new_filename = f"ECMA_{safe_title}.zip"
        target_path = os.path.join(DOWNLOAD_FOLDER, new_filename)
        shutil.move(downloaded_path, target_path)
        print(f"üì• Saved: {target_path}")

        # Upload to Dropbox
        try:
            dest_folder = DROPBOX_FOLDER_REPL if is_zenodo else DROPBOX_FOLDER_SUPP
            dbx_path = _upload_file_to_dropbox(target_path, dest_folder, new_filename)
            print(f"‚úÖ Uploaded to Dropbox: {dbx_path}")
            try:
                os.remove(target_path)
            except Exception:
                pass
        except Exception as e:
            print(f"‚ùå Dropbox upload failed; keeping local file: {e}")

        # Update Excel depending on source
        if is_zenodo:
            journal_data.at[orig_idx, FLAG_REPL_COL] = 1
            print(f"‚úÖ Marked {FLAG_REPL_COL} for row {orig_idx}")
        else:
            journal_data.at[orig_idx, FLAG_SUPP_COL] = 1
            print(f"‚úÖ Marked {FLAG_SUPP_COL} for row {orig_idx}")

        journal_data.to_excel(EXCEL_PATH, index=False)
        time.sleep(2)

    except Exception as e:
        print(f"[{orig_idx}] ‚ùå Error: {e}")
        continue

# =========================
# Cleanup
# =========================
driver.quit()
print("\nüéâ Process complete.")
